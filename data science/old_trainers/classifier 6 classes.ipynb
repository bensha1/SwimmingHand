{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Same as the active 4 class classifier but classifies into 6 classes and not 4.\n",
    "With six classes defining hand position for each class was problematic and the error was quite high so we narrowed it down to four.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #tf version 2.1.1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tinymlgen import port\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_model_classifier(data,epoch_count, neurons):\n",
    "    inp, output = [], []\n",
    "    for tup in data:\n",
    "        try:\n",
    "            inp.append((tup[0],tup[1],tup[2],tup[3],tup[4],tup[5]))\n",
    "            output.append(tup[6])\n",
    "        except:\n",
    "            print(\"problem tuple\")\n",
    "            pass\n",
    "    size = min(len(inp), len(output))\n",
    "    in1 = np.array(inp[:size])\n",
    "    out = np.array(output[:size])\n",
    "\n",
    "    # split into train, validation, test\n",
    "    TRAIN_SPLIT =  int(0.6 * size)\n",
    "    TEST_SPLIT = int(0.2 * size + TRAIN_SPLIT)\n",
    "    x_train, x_test, x_validate = np.split(in1, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "    y_train, y_test, y_validate = np.split(out, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "    # create a NN with 5 layers of 16 neurons\n",
    "    model = Sequential([\n",
    "    Dense(neurons, activation='relu', input_shape=(6,)),\n",
    "    Dense(neurons, activation='relu'),\n",
    "    Dense(neurons/2, activation='relu'),\n",
    "    Dense(6)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(x_train, y_train, verbose=0, epochs=epoch_count, batch_size=16,\n",
    "                        validation_data=(x_validate, y_validate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    labled_data = []\n",
    "    min_length = 100000\n",
    "    path = r\"C:\\Users\\Nadav\\Downloads\\log7-6_angle_{}.pkl\"\n",
    "    for label in [0, 60, 120, 180, 240, 300]:\n",
    "        with open(path.format(label), \"rb\") as f: \n",
    "            data = pickle.load(f)\n",
    "            if len(data) < min_length:\n",
    "                min_length = len(data)\n",
    "    for label in [0, 60, 120, 180, 240, 300]:\n",
    "        i = 0\n",
    "        with open(path.format(label), \"rb\") as f: \n",
    "            data = pickle.load(f)\n",
    "            for tup in data:\n",
    "                unlabled = list(tup[1:])\n",
    "                unlabled.append(int(label/60))\n",
    "                labled_data.append(unlabled)\n",
    "                i += 1\n",
    "                if (i == min_length):\n",
    "                    break\n",
    "    random.shuffle(labled_data)\n",
    "    return labled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_fit(tup):\n",
    "    i = 0\n",
    "    index = 0\n",
    "    max_val = -1\n",
    "    while (i < 6):\n",
    "        if (tup[i] > max_val):\n",
    "            index = i\n",
    "            max_val = tup[i]\n",
    "        i += 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(model, test_data):\n",
    "    inp, output = [], []\n",
    "    for tup in test_data:\n",
    "        try:\n",
    "            inp.append((tup[0],tup[1],tup[2],tup[3],tup[4],tup[5]))\n",
    "            output.append(tup[6])\n",
    "        except:\n",
    "            print(\"problem tuple\")\n",
    "            pass\n",
    "    size = min(len(inp), len(output))\n",
    "    in1 = np.array(inp[:size])\n",
    "    out = np.array(output[:size])\n",
    "\n",
    "    # split into train, validation, test\n",
    "    TRAIN_SPLIT =  int(0.6 * size)\n",
    "    TEST_SPLIT = int(0.2 * size + TRAIN_SPLIT)\n",
    "    x_train, x_test, x_validate = np.split(in1, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "    y_train, y_test, y_validate = np.split(out, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "    \n",
    "    tries = 0.0\n",
    "    num_correct = 0.0\n",
    "    \n",
    "    for i in range(len(x_validate)):\n",
    "        pred = model.predict(np.array([(x_validate[i][0],x_validate[i][1],x_validate[i][2],x_validate[i][3],x_validate[i][4],x_validate[i][5])]))[0]\n",
    "        res = best_fit(pred)\n",
    "        tries += 1\n",
    "        if (res == y_validate[i]):\n",
    "            num_correct += 1\n",
    "    percent_correct = num_correct/tries*100\n",
    "    print(\"percent correct: {}\".format(percent_correct))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem tuple\n",
      "problem tuple\n"
     ]
    }
   ],
   "source": [
    "labled = create_data()\n",
    "model = hand_model_classifier(labled, 100, 16)\n",
    "c_code = port(model, pretty_print=True).replace(\"model_data\", \"hand_model\")\n",
    "model_str = \"model_{}_{}_{}_classifier_error_{}\".format(0, 30, 16, int (error(model, labled)))\n",
    "open(r\"C:\\Users\\Nadav\\Documents\\IoT project\\models\\{}.h\".format(model_str), \"w\").write(c_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
